---
title: "Мини-проект по анализу корпуса текстов"
author: "Горчинская Арина, Турутин Никита, Астапенкова Арина"
output:
  html_document:
    df_print: paged
---

```{r message=FALSE}
# библиотеки
library(tidyverse)
library(dplyr)
library(ggplot2)
library(tidyr)
library(readr)
library(stringr)
library(forcats)
library(purrr)

library(tidytext)
library(stopwords)
#install.packages("udpipe")
library(udpipe)
library(topicmodels)
library(quanteda)
library(quanteda.textstats)
#install.packages("widyr")
library(widyr)
library(ggraph)
library(igraph)
```

## Описание данных

Данные содержат собранные в txt-файлы книгу "Мастер и Маргарита", написанную Булгаковым. В данных 3 столбца:

- `chapter_number` - номер главы

- `chapter_name` - название главы

- `text` - текст.

```{r}
data <- read_csv('corpus_chapters.csv', show_col_types = FALSE) |> 
  mutate(part = if_else(chapter_number <= 16, "первая", "вторая"), .before = 1) # добавление колонки с частью книги
head(data)
```

## Предобработка

Токенизация, лемматизация

```{r}
# загружаем предобученную модель для русского языка (модель предварительно сохранена локально)
# ud_model <- udpipe_download_model(language = "russian") --- первое скачивание
ud_model <- udpipe_load_model("russian-gsd-ud-2.5-191206.udpipe")
```

```{r}
# стоп-слова
stopwords <- stopwords("ru", source = "snowball")
m_words <- c("это", "все", "свой", "который", "еще", "чтоб", "вот", "как", "то", "тут", "или", "того", "этот", "тот", "очень", "весь", "стать", "говорить", "ответить")
stop_list <- unique(c(stopwords, m_words))

# фильтрация
token_words <- data |>
  mutate(text = str_replace_all(text, "ё", "е")) |>
  mutate(anno = map(text, ~ as_tibble(udpipe_annotate(ud_model, x = .x)))) |>
  unnest(anno) |>
  mutate(lemma = str_to_lower(lemma)) |>
  filter(str_detect(lemma, "^[а-я-]+$")) |>
  filter(!lemma %in% stop_list, nchar(lemma) > 2)
```

## Частотный анализ

Частота по всему корпусу:

```{r}
frequencies_1 <- token_words |> count(lemma, sort = TRUE)
frequencies_1
```



Визуализируем **топ-20 слов** (выделяя имена персонажей)

```{r}
# список персонажей для выделения
characters <- c("маргарита", "воланд", "иван", "азазелло",
                "коровьев", "кот", "пилат", "левий")

frequencies_1 |>
  slice_max(n, n = 20) |>
  mutate(type = ifelse(lemma %in% characters, "персонаж", "другое")) |>
  ggplot(aes(x = fct_reorder(lemma, n), y = n, fill = type)) +
  geom_col(color = "darkred", show.legend = FALSE) +
  coord_flip() +
  scale_fill_manual(values = c("персонаж" = "firebrick", "другое" = "lightcoral")) +
  labs(x = NULL, y = "Частота", title = "Топ-20 слов по корпусу") +
  theme_minimal(base_family = "serif") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5, color = "darkred"),
    axis.text = element_text(size = 10, color = "darkred"),
    axis.title = element_text(size = 11, color = "darkred")
  )
```

Анализ показал преобладание лемм, связанных с именами персонажей (маргарита, иван, воланд, кот) и частями тел (рука, глаз, голова, лицо). Это говорит нам о том, что в тексте концентрируется внимание на героях, описании их мимики и жестов, что отражает жанр текста - роман. Маргарита, Иван, Воланд, Коровьев, Кот - это имена, которые встречаются наиболее часто, потому что они значимы для повествования.

Частота по главам:

```{r}
frequencies_2 <- token_words |> count(chapter_number, chapter_name, lemma, sort = TRUE)

frequencies_2 |> 
  group_by(chapter_number, chapter_name) |>
  slice_max(n, n = 1) |>
  ungroup()
```


Визуализация топ-1 слова в каждой главе

```{r}
frequencies_2 |> 
  group_by(chapter_number, chapter_name) |>
  slice_max(n, n = 1) |>
  ungroup() |>
  mutate(row_color = if_else(row_number() %% 2 == 0, "lightcoral", "firebrick")) |>
  ggplot(aes(x = reorder(chapter_name, chapter_number), y = n, fill = row_color)) +
  geom_col(color = "brown", alpha = 0.8, show.legend = FALSE) +
  geom_text(aes(label = lemma), 
            position = position_stack(vjust = 0.5), 
            size = 3, 
            color = "white",
            fontface = "bold") +
  scale_fill_identity() +
  coord_flip() +
  labs(x = "Глава", y = "Частота", 
       title = "Самое частотное слово в каждой главе") +
  theme_minimal(base_family = "serif") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5, color = "darkred"),
    axis.text = element_text(size = 9, color = "darkred"),
    axis.title = element_text(size = 11, color = "darkred"),
    axis.text.y = element_text(size = 8)
  )
```


На графике видно, что в разных главах лидируют разные персонажи или ключевые понятия, соответствующие основной сцене. Например, главы с московскими событиями характеризуются доминированием слов вроде «иван», «берлиоз», «воланд», тогда как в главах библейского цикла ключевым словом становится «прокуратор» или «иешуа».

## Регулярные выражения 

Подсчет частоты упоминания **всех персонажей** по главам + нормированная частота на 1000 слов текста

```{r}
persons <- c(
  "воланд[ауом]?|мессир(у|ом)?|профессор(у|ом|а)?",
  "маргарит[аыуойе]|ма[р]+гарит[аыуойе]",
  "мастер[ау]?",
  "иван[ауом]?|бездомн(ый|ого|ому|ым)?|понырев[ау]?",
  "азазелл[оау]",
  "коровь[её]в[ау]?|фагот(у|ом|а)?",
  "бегемот[ауом]?|кот[ауом]?",
  "пилат[ауом]?|понтий|прокуратор[ауом]?",
  "иешуа|га-ноцри",
  "левий|матвей",
  "берлиоз[ауом]?|михаил[ауом]?|александрович[ауом]?",
  "латунск[а-я]+",
  "алоизий|могарыч"
)

regex <- paste0("\\b(", paste(persons, collapse="|"), ")\\b")

persons_ <- data |>
  mutate(text_norm = stringr::str_to_lower(stringr::str_replace_all(text, "ё","е")),
         hits = stringr::str_count(text_norm, regex), # частота упоминаний
         words_total = str_count(text_norm, "\\S+"),
         hits_norm = hits / words_total * 1000) # частота упоминаний на 1000 слов текста

persons_ |> 
  select(part, chapter_number, chapter_name, hits, hits_norm) |> 
  arrange(desc(hits))
```

```{r}
ggplot(persons_, aes(x = fct_reorder(chapter_name, hits), y = hits, fill = part)) +
  geom_col(color = "white") +
  coord_flip() +
  scale_fill_manual(values = c("первая" = "firebrick", "вторая" = "#009999")) +
  theme_minimal(base_family = "serif") +
  labs(x = "Глава", y = "Количество упоминаний", 
       title = "Упоминания персонажей по главам",
       fill = "Часть романа") +
  theme(legend.position = "bottom")
```

Наибольшее количество упоминаний приходится на главы с активным развитием сюжета (24, 2, 22, 23) и множеством диалогов — именно в них повествование сосредоточено на взаимодействии героев. Можно заметить, что в 20х главах происходит наибольшее количество взаимодействий персонажей, что говорит нам о кульминации событий романа — моментом, когда в повествовании одновременно присутствует несколько ключевых персонажей и развивается линия Маргариты.

```{r}
ggplot(persons_, aes(x = fct_reorder(chapter_name, hits_norm), y = hits_norm, fill = part)) +
  geom_col(color = "white") +
  coord_flip() +
  scale_fill_manual(values = c("первая" = "firebrick", "вторая" = "#009999")) +
  theme_minimal(base_family = "serif") +
  labs(x = "Глава", y = "Количество упоминаний на 1000 слов текста", 
       title = "Упоминания персонажей по главам (нормированное)",
       fill = "Часть романа") +
  theme(legend.position = "bottom")
```

Нормированное распределение позволяет увидеть, что наибольшая концентрация взаимодействий между персонажами приходится именно на центральные главы романа, где происходит развёртывание ключевых событий.

## Log-likelihood / log-ratio

```{r}
# Создаем корпус и документ-термин матрицу
corp <- corpus(data, text_field = "text")
toks <- tokens(corp, remove_punct = TRUE) |>
  tokens_tolower() |>
  tokens_replace(pattern = "ё", replacement = "е") |>
  tokens_keep(pattern = "^[а-я-]+$", valuetype = "regex") |>
  tokens_remove(stop_list)

dfm_all <- dfm(toks)

# Добавляем метаданные о частях романа
docvars(dfm_all, "part") <- data$part

# Log-likelihood анализ
key_ll <- textstat_keyness(dfm_all, 
                          target = docvars(dfm_all, "part") == "первая", 
                          measure = "chi2")
head(key_ll, 15)
```

```{r}
# Визуализация Log-likelihood (топ-15 слов для каждой части)
key_ll_top <- bind_rows(
  key_ll |> filter(chi2 > 0) |> slice_max(chi2, n = 15),  # слова для первой части
  key_ll |> filter(chi2 < 0) |> slice_min(chi2, n = 15)   # слова для второй части
) |>
  mutate(type = ifelse(chi2 > 0, "первая часть", "вторая часть"))

ggplot(key_ll_top, aes(x = fct_reorder(feature, chi2), y = chi2, fill = type)) +
  geom_col(show.legend = TRUE) +
  coord_flip() +
  scale_fill_manual(values = c("первая часть" = "firebrick", "вторая часть" = "#009999")) +
  labs(x = NULL, y = "Log-likelihood (chi2)",
       title = "Ключевые слова по частям романа (Log-likelihood)",
       fill = "Часть романа") +
  theme_minimal(base_family = "serif") +
  theme(legend.position = "bottom")
```

```{r}
# Log-ratio анализ - получаем результаты для обеих частей
key_lr <- textstat_keyness(dfm_all, 
                          target = docvars(dfm_all, "part") == "первая",
                          measure = "lr")
head(key_lr, 20)
```

```{r}
# Визуализация Log-ratio (топ-15 слов для каждой части)
key_lr_top <- bind_rows(
  key_lr |> filter(G2 > 0) |> slice_max(G2, n = 15),  # слова для первой части
  key_lr |> filter(G2 < 0) |> slice_min(G2, n = 15)   # слова для второй части
) |>
  mutate(type = ifelse(G2 > 0, "первая часть", "вторая часть"))

ggplot(key_lr_top, aes(x = fct_reorder(feature, G2), y = G2, fill = type)) +
  geom_col(show.legend = TRUE) +
  coord_flip() +
  scale_fill_manual(values = c("первая часть" = "firebrick", "вторая часть" = "#009999")) +
  labs(x = NULL, y = "Log-ratio (G2)",
       title = "Ключевые слова по частям романа (Log-ratio)",
       fill = "Часть романа") +
  theme_minimal(base_family = "serif") +
  theme(legend.position = "bottom")
```

В первой части романа стастистически значимо чаще встречаются имена Иван, Никанор, Веренуха, которые являются персонажами Московской сюжетной линии. Во второй части романа чаще встречаются имена Маргарита, Азазелло, Воланд, которые являются персанажами мистической сюжетной линии. 

## Семантическая сеть

```{r warning=FALSE}
bigrams <- data |>
  mutate(text = str_replace_all(text, "ё", "е")) |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("w1", "w2"), sep = " ") |>
  filter(
    str_detect(w1, "^[а-я-]+$"),
    str_detect(w2, "^[а-я-]+$"),
    !w1 %in% stop_list,
    !w2 %in% stop_list
  )

edges <- bigrams |>
  count(w1, w2, sort = TRUE) |>
  filter(n >= 5)   

# функция для тестирования порогов
test_thresholds <- function() {
  thresholds <- c(2, 5, 8, 10, 15, 20, 25)
  
  for(thresh in thresholds) {
    edges_test <- edges |> filter(n >= thresh)
    
    cat(sprintf("Порог %2d: %4d связей, %3d уникальных слов | ",
                thresh, nrow(edges_test), 
                n_distinct(c(edges_test$w1, edges_test$w2))))
    
    if(nrow(edges_test) > 0) {
      g_test <- graph_from_data_frame(edges_test, directed = FALSE)
      cat(sprintf("Плотность: %.4f", edge_density(g_test)))
    }
    cat("\n")
  }
}

test_thresholds()

# граф 
g <- graph_from_data_frame(edges, directed = FALSE)
set.seed(30)
ggraph(g, layout = "fr") +
  geom_edge_link(alpha = .8, color = "#009999", width = 0.7) + 
  geom_node_point(color = "firebrick", size = 3, alpha = 0.8) + 
  geom_node_text(aes(label = name), repel = TRUE, size = 3, color = "darkblue") +
  labs(title = "Семантическая сеть биграмм") +
  theme_void()
```

## Тематическое моделирование (LDA)

Подготовка данных и построение модели LDA

```{r message=FALSE}
dtm_tbl <- token_words |> 
  filter(!is.na(lemma)) |> 
  count(document = as.character(chapter_number), term = lemma, sort = TRUE)

# преобразуем в объект типа Document-Term-Matrix
dtm <- dtm_tbl |> 
  cast_dtm(document, term, n)

# параметры LDA
k <- 10 
seed <- 1234
control <- list(seed = seed)

# Обучаем модель
lda_model <- LDA(dtm, k = k, method = "Gibbs", control = control)
```

Топ-термы по темам (beta)

```{r}
topics_beta <- tidy(lda_model, matrix = "beta")

top_n <- 10
top_terms <- topics_beta |>
  group_by(topic) |>
  slice_max(beta, n = top_n) |>
  ungroup() |>
  arrange(topic, -beta)

top_terms |>
  group_by(topic) |>
  summarize(terms = paste(term, collapse = ", ")) |>
  ungroup()
```

Визуализация: топ-термы по темам (гистограммы)

```{r}
red_palette <- c("lightcoral", "indianred", "brown", "firebrick", "darkred", 
                 "maroon", "coral", "tomato", "orangered", "red3")
top_terms |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  scale_fill_manual(values = red_palette) + 
  labs(title = "Топ-слова по темам",
       x = NULL, y = "Вероятность (beta)") +
  theme_minimal(base_family = "serif")
```

На основе топ-слов, представленных в гистограммах, Можно выделить 10 тем: 
1. Свита Воланда и комические сцены 
2. Бюрократическая и городская среда 
3. Московская завязка, персонажи начальных эпизодов
4. Театр, сцена и публичные выступления
5. Описательная лексика
6. Иван Бездомный и московская линия
7. Библейская линия
8. Маргарита и центральные мистические эпизоды
9. Сюжетная ветвь Понтия Пилата
10. Маргарита и ночные мотивы

Распределение тем по главам

```{r}
topics_gamma <- tidy(lda_model, matrix = "gamma") |>
  group_by(document) |>
  slice_max(gamma, n = 1) |>
  ungroup() |>
  mutate(document = as.numeric(document))

# Добавляем информацию о главах
chapters_info <- data |>
  select(chapter_number, chapter_name, part) |>
  distinct() |> 
  mutate(document = as.character(chapter_number))

topics_with_chapters <- topics_gamma |>
  left_join(chapters_info, by = c("document" = "chapter_number"))

topics_with_chapters |> select(part, document, chapter_name, topic, gamma)
```

Визуализация доминирующих тем по главам

```{r}
ggplot(topics_with_chapters, aes(x = factor(document), y = gamma, fill = factor(topic))) +
  geom_col() +
  facet_grid(part ~ ., scales = "free", space = "free") +
  scale_fill_manual(values = red_palette, name = "Тема") +
  labs(title = "Доминирующие темы по главам",
       x = "Номер главы", y = "Доля темы", fill = "Тема") +
  theme_minimal(base_family = "serif") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Интерпретация тем на основе топ-слов и принадлежности к главам**

```{r}
topics_interpretation <- top_terms |>
  group_by(topic) |>
  summarise(
    top_words = paste(head(term, 10), collapse = ", "),  # топ-10 слов для каждой темы
    .groups = "drop"
  )

# Примеры глав для каждой темы
example_chapters <- topics_with_chapters |>
  group_by(topic) |>
  slice_max(gamma, n = 3) |>
  select(topic, chapter_number = document, chapter_name, gamma) |>
  arrange(topic, desc(gamma))

# Все данные
interpretation_summary <- example_chapters |>
  left_join(topics_interpretation, by = "topic") |>
  select(topic, top_words, chapter_number, chapter_name, gamma) |>
  arrange(topic, desc(gamma))

interpretation_summary
```

Результаты LDA показывают, что в тексте выделяется несколько устойчивых тематических кластеров, каждый из которых соответствует определённым сюжетным линиям романа. Моделирование подтверждает структуру романа (Московская линия, мистическая линия, библейская линия), в которой темы отражают композиционные блоки и смену повествовательных миров.

---
title: "Мини-проект по анализу корпуса текстов"
output: html_notebook
---

```{r message=FALSE}
# библиотеки
library(tidyverse)
library(stopwords)
library(tidytext)
library(stopwords)  
library(stringr)
library(ggplot2)
library(forcats)
library(quanteda)
library(quanteda.textstats)
library(widyr)
library(igraph)
library(ggraph)
library(dplyr)
library(tidyr)
# install.packages("udpipe")
library(udpipe)
```

## Описание данных

Данные содержат собранные в .txt файлы книгу "Мастер и Маргарита", написанного Булгаковым. В данных 4 столбца: 1 столбец - название файла, где содержаться данные, второй столбец - номер главы, третий - название главы, четвертый столбец - текст.

```{r}
data <- read_csv('corpus_chapters.csv') |>
  select(-file)
```

```{r}
head(data)
```

## Предобработка

Токенизация, лемматизация

```{r}
# загружаем предобученную модель для русского языка (делается один раз)
# модель сохраняется локально
ud_model <- udpipe_download_model(language = "russian")
ud_model <- udpipe_load_model(ud_model$file_model)
```

```{r}
# стоп-слова
stopwords <- stopwords("ru", source = "snowball")
m_words <- c("это", "все", "свой", "который", "еще", "чтоб", "вот", "как", "то", "тут", "или", "того", "этот", "тот", "очень", "весь", "стать")
summary <- unique(c(stopwords, m_words))

# фильтрация
token_words <- data |>
  mutate(text = str_replace_all(text, "ё", "е")) |>
  mutate(anno = map(text, ~ as_tibble(udpipe_annotate(ud_model, x = .x)))) |>
  unnest(anno) |>
  mutate(lemma = str_to_lower(lemma)) |>
  filter(str_detect(lemma, "^[а-я-]+$")) |>
  filter(!lemma %in% summary, nchar(lemma) > 2)

# частоты по всему корпусу и главам
frequencies_1 <- token_words |> count(lemma, sort = TRUE)
frequencies_2 <- token_words |> count(chapter, lemma, sort = TRUE)
```

Частота по всему корпусу:

```{r}
frequencies_1
```

Частота по главам:

```{r}
frequencies_2
```

Визуализируем **топ-20 слов**

```{r}
# список персонажей для выделения
characters <- c("маргарита","воланд","иван","азазелло",
                "коровьев","бегемот","пилат","левий")

frequencies_1 |>
  slice_max(n, n = 20) |>
  mutate(type = ifelse(lemma %in% characters, "персонаж", "другое")) |>
  ggplot(aes(x = fct_reorder(lemma, n), y = n, fill = type)) +
  geom_col(color = "darkred", show.legend = FALSE) +
  coord_flip() +
  scale_fill_manual(values = c("персонаж" = "firebrick", "другое" = "lightcoral")) +
  labs(x = NULL, y = "Частота", title = "20 слов по корпусу") +
  theme_minimal(base_family = "serif") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5, color = "darkred"),
    axis.text = element_text(size = 10, color = "darkred"),
    axis.title = element_text(size = 11, color = "darkred")
  )
```
RegEx - регулярные выражения упоминания персонажей по главам

```{r}
# RegEx
persons <- c("воланд","маргарит[аиы]","бездомн[а-я]+","берлиоз","понырев","пилат","левий","азазелло","бегемот")

regex <- paste0("\\b(", paste(persons, collapse="|"), ")\\b")

persons_ <- data |>
  mutate(text_norm = stringr::str_to_lower(stringr::str_replace_all(text, "ё","е")),
         hits = stringr::str_count(text_norm, regex)) |>
  select(chapter, hits) |>
  arrange(desc(hits))
persons_ |> head(10)
```

```{r}
chapters_ <- data |>
  mutate(ch_num = readr::parse_number(chapter),
         part = if_else(ch_num <= 16, "первая", "вторая"))

corp <- corpus(chapters_, text_field = "text")
toks <- tokens(corp, remove_punct = TRUE) |>
  tokens_tolower() |>
  tokens_replace(pattern = "ё", replacement = "е") |>
  tokens_keep(pattern = "^[а-я-]+$", valuetype = "regex") |> 
  tokens_remove(summary)

dfm_all <- dfm(toks)

# Log-likelihood 
key_ll <- textstat_keyness(dfm_all, 
                           target = docvars(dfm_all, "part") == "первая", 
                           measure = "chi2")
head(key_ll, 10)


# Log-ratio 
key_lr   <- textstat_keyness(dfm_all, target = docvars(dfm_all, "part") == "первая"
                             , measure = "lr")
head(key_lr, 15)
```

```{r}
data_g <- key_ll |> slice_head(n = 15)

ggplot(data_g, aes(x = reorder(feature, chi2), y = chi2, fill = ifelse(n_target > n_reference, "первая", "Вторая"))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, y = "log-likelihood (chi2)",
       title = "Ключевые слова (LL)")
```

```{r}
# семантическая сеть

bigrams <- data |>
  mutate(text = str_replace_all(text, "ё", "е")) |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("w1", "w2"), sep = " ") |>
  filter(
    str_detect(w1, "^[а-я-]+$"),
    str_detect(w2, "^[а-я-]+$"),
    !w1 %in% summary,
    !w2 %in% summary
  )

edges <- bigrams |>
  count(w1, w2, sort = TRUE) |>
  filter(n >= 5)   

# функция для тестирования порогов
test_thresholds <- function() {
  thresholds <- c(2, 5, 8, 10, 15, 20, 25)
  
  for(thresh in thresholds) {
    edges_test <- edges |> filter(n >= thresh)
    
    cat(sprintf("Порог %2d: %4d связей, %3d уникальных слов | ",
                thresh, nrow(edges_test), 
                n_distinct(c(edges_test$w1, edges_test$w2))))
    
    if(nrow(edges_test) > 0) {
      g_test <- graph_from_data_frame(edges_test, directed = FALSE)
      cat(sprintf("Плотность: %.4f", edge_density(g_test)))
    }
    cat("\n")
  }
}

test_thresholds()

# граф 
g <- graph_from_data_frame(edges, directed = FALSE)
set.seed(30)
ggraph(g, layout = "fr") +
  geom_edge_link(alpha=.3) +
  geom_node_point() +
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +
  labs(title = "Семантическая сеть биграмм")
```

